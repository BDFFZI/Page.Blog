# 【人工智能】 模型优化

## 模型属性

- 拟合：指模型对数据集中数据的预测能力。
- 泛化：指模型对数据集之外数据的预测能力。

## 拟合问题

- 欠拟合（高偏差）：模型拟合能力差（如用直线拟合曲线数据）。
- 过拟合（高方差）：模型拟合能力非常好，但不能泛化（曲线变化率过高，经常给自己“加戏”）。

### 解决过拟合

- 增加训练数据
- 减少输入特征
- 正则化模型参数

## 正则化参数

正则化模型参数的本质就是将模型参数也记录到成本中，从而使机器学习中尽可能减少对某些参数的使用，变相的减少了输入特征的权重。正则化可以对部分参数使用，但对所有参数使用时它也同样能解决过拟合问题。

正则化的成本函数如下：

$$
J= \frac{1}{m}\sum_{i=1}^m L^{(i)}+\frac{\lambda}{m}\sum_{j=1}^{n}\frac{1}{2}w_j^2
$$

- $n$：模型参数数量。
- $\lambda$：正则化参数，越高正则化效果越明显，但过高，参数将无法被训练。

除 m 是因为增加训练数据也可以减少过拟合，因此当数据变多后正则化参数也应减少，以确保效果前后一致，此外这也可以实现与前一项公式的统一。乘 2 则是与平方误差损失函数一样，为了后续简化参数。

同样的，此时模型的梯度下降公式将改为：

$$
w_j=w_j-a(\frac{1}{m}\sum_{i=1}^mL'+\frac{\lambda}{m}w_j)
$$

如果将该公式进一步展开，还可以观察到正则化的工作原理：

$$
w_j=w_j(1-a\frac{\lambda}{m})-a\frac{1}{m}\sum_{i=1}^mL'
$$

对比原始的梯度下降公式，唯一的变化就是每次迭代时给模型参数额外乘上了一个系数 $1-a\frac{\lambda}{m}$，可以推测的是该参数始终是一个略小于 1 的值，这意味着每次迭代时，正则化的梯度下降将会略微缩小相关参数。
