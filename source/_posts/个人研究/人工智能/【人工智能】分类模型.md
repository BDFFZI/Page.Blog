# 【人工智能】分类模型

## 逻辑函数

逻辑函数是分类模型中常用的一种数据处理函数，其能实现将任意输入参数，以 0 为分界线转换到 0-1 的范围，以此便实现了将回归模型的技术放在分类模型中使用。

$$
g(z) = \frac{1}{1+e^{-z}}
$$

## 逻辑回归模型

逻辑回归用于判断单个事件的发生概率，其回归模型如下：

$$
f(x)=g(\vec{w}\cdot\vec{x}+b)=\frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}
$$

本质就是线性函数与逻辑函数的组合，其中线性函数确定了分类范围，而逻辑函数使其输出结果转为 0-1 的概率（所有随机事件的概率相加等于 1）。

此外逻辑回归模型还有另一种写法，以体现其输出为概率的思想：

$$
f(x)=P(y=1|\vec{x};\vec{w},b)
$$

- $\vec{w},b$：模型参数
- $\vec{x}$：输入特征

### 决策边界

逻辑回归模型中的回归函数用于确定分类范围，特别的当其值为 0 时，该范围被称为逻辑边界，此时的概率恰好为 0.5，可以将其作为判断结果为“是”、“否”的分界线。

例如当模型为 $g(x_1^2+x_2^2-1)$ ，其输出为 0.5 时：

$
\displaystyle
\begin{aligned}
g(x_1^2+x_2^2-1) &= 0.5 \\
x_1^2+x_2^2-1 &= 0 \\
x_1^2+x_2^2 &= 1
\end{aligned}
$

其边界形状恰好是一个圆。所以输入参数在该圆上的位置和距离决定了回归函数的输出，然后逻辑函数再将其结果转为概率。

### 多标签分类

在输出层使用逻辑回归模型，不代表结果只能是 0、1，因为输出可以有多个，因此可以输出多个相互独立的概率，这种分类问题有一个特别的名字叫“多标签分类”。

## 二元交叉熵损失函数

平方误差损失函数不适用分类模型，这样产生函数会有多个极值成台阶状（因为分类模型的结果是离散的），故此处有专门适用于分类模型的损失函数:

$$
L = lerp(-log(1-\hat{y}),-log(\hat{y}),y)=-(1-y)log(1-\hat{y})-ylog(\hat{y})
$$

- 注意：此处 $\hat{y} \in \{0,1\}$

$-log$ 在此处的值域为$[+\infin,0]$，所以当$y=1,\hat{y}=1$ 时，$-log(\hat{y})=0$ 即没有误差，反之依然。

### 二元交叉熵损失函数求导

根据链式法则有：

$\displaystyle \frac{dL}{dw} = \frac{dL}{dg}\frac{dg}{dz}\frac{dz}{dw}$

$\displaystyle \frac{dL}{db} = \frac{dL}{dg}\frac{dg}{dz}\frac{dz}{db}$

依次求解可得：

$
\begin{aligned}
\frac{dL}{dg} 
&= (-(1-y)log(1-g)-ylog(g))' \\
&= \frac{1-y}{1-g}-\frac{y}{g} \\
&= \frac{(1-y)g-y(1-g)}{(1-g)g}\\
&= \frac{g-yg-y+yg}{(1-g)g}\\
&= \frac{g-y}{(1-g)g}
\end{aligned}
$

$
\begin{aligned}
\frac{dg}{dz}
&= (\frac{1}{1+e^{-z}})'\\
&= ((1+e^{-z})^{-1})' * (1+e^{-z})' \\
&= -(1+e^{-z})^{-2} * -e^{-z} \\
&= \frac{e^{-z}}{(1+e^{-z})^2} \\
&= \frac{e^{-z}}{1+e^{-z}} * \frac{1}{1+e^{-z}} \\
&= \frac{1+e^{-z}-1}{1+e^{-z}} g \\
&= (1-g)g
\end{aligned}
$

$\displaystyle \frac{dL}{dg}\frac{dg}{dz} = \frac{g-y}{(1-g)g}(1-g)g=g-y$

$\displaystyle \frac{dz}{dw} = (wx+b)'= x$

$\displaystyle \frac{dz}{db} = (wx+b)'= 1$

故最终可得：

$\displaystyle \frac{dL}{dw}=(g-y)x = (\hat{y}-y)x$

$\displaystyle \frac{dL}{db}=(g-y) = \hat{y}-y$

## Softmax 回归模型

Softmax 回归是逻辑回归的多维泛化版本（二维 Softmax 等于逻辑回归），用于实现从多个类型中预测结果，其回归函数如下：

$$
z_j = \vec{w}\cdot\vec{x}+b_j~|~j=1,\dots,N
$$

$$
a_j=\frac{e^{z_j}}{\sum_{k=1}^{N}e^{z_k}}=P(y=j|\vec{x})
$$

- N：预测结果的类型数量

可见 Softmax 会输出多个结果，而每个结果都是预测为该分类的概率，所有概率相加等于 1。这种多中选一的问题，叫做“多类分类问题”。

## 稀疏分类交叉损失函数

这是 Softmax 采用的损失函数，其计算方法与逻辑回归类似：

$$
L(\vec{a},y)=-log(a_y)
$$
