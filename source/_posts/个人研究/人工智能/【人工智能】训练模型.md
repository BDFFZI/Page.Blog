# 【人工智能】训练模型

训练模型关键是两点：

1. 找出模型的成本函数
2. 找到使成本最小的模型参数

## 损失函数

损失函数用于描述模型在单个样本上的误差值，本质就是成本函数中求和时的单位元素。由于成本函数中累加和平均的过程是线性变化，因此决定成本函数形状的主要是损失函数。损失函数的选择，将决定成本函数的效果。

$$
L(f(x^{(i)}),y^{(i)})
$$

损失函数有很多种选择，但其目的都是为了使成本函数能成为凸函数，从而使其能被梯度下降来求解最小值。

## 成本函数

成本函数用于确定基于模型计算的预估值与整个数据集实际值之间的大致误差，误差越大表明模型参数越不准确。将所有样本的损失函数结果相加即可得到成本函数。

损失函数有很多种，所以成本函数也有很多种。其针对不同模型的效果也不同，因此要根据具体的模型性质选择成本函数，否则会导致难以求导，或无法计算最小成本的情况。

$$
J= \frac{1}{m}\sum_{i=1}^m L^{(i)}
$$

- 取平均是为了避免计算结果因数据集大小导致的数值过大。

## 梯度下降

有了成本函数后就要开始寻找使成本函数值最小的模型参数，其中一种常用方法便是梯度下降。

梯度下降的本质就是对成本函数求导，再让模型参数以导数的反方向移动，重复多次直到当导数收敛，此时的模型参数值便是最符合数据集的选择。

其原理也很容易理解：因为导数决定了函数值的变化方向，因此让模型参数沿导数的反方向移动即可抵达函数的极值点。若选择成本函数时，确保了成本函数只有一个为最小值的极值点（非凸代价函数），那梯度下降最终抵达的位置一定是最小成本点。

$$
w_i = w_i - a\frac{d}{dw_i}J
$$

- 学习率（a）：梯度下降的速率（过小或过大会导致模型训练过慢或无法收敛）。
- $w_i$：任意模型参数（故梯度下降中的导数实际是偏导数）

由于成本函数只是损失函数的平均，不会影响损失函数的求导结果，故也可以简写为：

$$
w_i = w_i-a\frac{1}{m}\sum_{i=1}^{m}L'
$$

注意：当 w 存在多个时，由于会计算多次，应注意缓存旧 w，以确保所有参数计算时的环境一致。

### 收敛判断

判断模型训练是否完成有两种方法：

1. 绘制学习曲线：以迭代次数为横轴，成本为纵轴，观察曲线变化方向是否逐渐变小且趋于稳定。
2. 自动收敛测试：设定一个最小成本差值，每次迭代判断成本变化是否小于该值，小于则说明收敛。

## 自适应矩估计

自适应矩估计（Adam）是一种自适应的学习率优化算法，其会给模型的每个参数分配不同的学习率，并根据参数的变化方向，如始终同向前进或震荡，自动增加或减少学习率。以提高梯度下降的速度和稳定性。

## 模型评估

### 基准性能水平

用于测量模型能力的基线，例如以人类处理相同任务的平均水平做对比。

### 偏差和方差

为了实现模型评估，通常会将数据集会为以下三类：

- 训练集：用于实践训练模型，确认模型的权重和偏置。
- 开发集（交叉验证集）：用于验证模型的方差情况。
- 测试集：用于最终测试模型能力。

通过评估这三类数据集的成本，并比较基准性能水平，就可以检测出模型的偏差和方差情况。一般来说，模型存在以下三种状态：

- 偏差过高，方差过高（神经元太少，欠拟合）
- 偏差过低，方差过高（神经元太多，过拟合）
- 偏差低，方差低（正正好）

其中偏差体现在训练集的成本上（$J_{train}$），方差体现在验证集的成本上（$J_{cv}$）。